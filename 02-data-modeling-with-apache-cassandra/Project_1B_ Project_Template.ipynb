{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace\n"
     ]
    }
   ],
   "source": [
    "# Checking current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# For every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# Reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "# Extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "\n",
    "# Creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create KEYSPACE \"sparkify\"\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "        WITH REPLICATION = \n",
    "        { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\n",
    "    \"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set KEYSPACE \"sparkify\" as current\n",
    "try:\n",
    "    session.set_keyspace('sparkify')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create table for Query 1:  Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "\n",
    "## PRIMARY KEY (PK) selection explanations: \\\n",
    "## First of all, PK should be unique. SessionId and ItemInSession can uniquely identify the row in the table, because our query \\\n",
    "## requires information only from the session and sessionItem, moreover the sessionItem is unique inside the session. \\\n",
    "## Here we can choose how to partition our data: by sessionId only, or by sessionId and itemInSession. \\\n",
    "## Both, approaches are acceptable, but in case of partitioning by sessionId only you can get into situation when some sessions \\\n",
    "## is very active (with high amount of itemInSession) and all of them will landed in the single partition which gives your a hot spot. \\\n",
    "## In the other hand, partitioning by both sessionId and itemInSession gives you more uniform distribution and this is my choice.\n",
    "createTableForQuery1 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS music_app_sessions(\n",
    "        sessionId int,\n",
    "        itemInSession int,\n",
    "        artist text,\n",
    "        song text,\n",
    "        song_length decimal,\n",
    "        PRIMARY KEY ((sessionId, itemInSession)));\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(createTableForQuery1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "insertDataForQuery1 = \"\"\"\n",
    "    INSERT INTO music_app_sessions(sessionId, itemInSession, artist, song, song_length)\n",
    "    VALUES(%s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader) # skip header\n",
    "        for line in csvreader:\n",
    "            # Choose required fields and convert them to proper datatypes for insertion\n",
    "            session.execute(insertDataForQuery1, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into table for Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Artist                             Song    Length\n",
      "0  Faithless  Music Matters (Mark Knight Dub)  495.3073\n"
     ]
    }
   ],
   "source": [
    "## Verify query 1: Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "selectForQuery1 = \"\"\"\n",
    "    SELECT artist, song, song_length\n",
    "    FROM music_app_sessions\n",
    "    WHERE sessionId = 338\n",
    "    AND itemInSession = 4;\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(selectForQuery1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "column_labels = ('Artist', 'Song', 'Length')\n",
    "result_df = pd.DataFrame(list(rows), columns=column_labels)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### COPY AND REPEAT THE ABOVE THREE CELLS FOR EACH REMAINING QUERY\n",
    "_Question_: Why should we repeat code for insertion and scan CSV file three times instead of single pass?\n",
    "\n",
    "_Answer_: Because we can have errors on the insertion statements, for example, in the table 3 and this means, that all tables will have uncompleted data. This is bad idea, because tables shouldn't affect each other and business should be able to query against other two tables while you fix the broken insert statement on table 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create table for Query 1: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "\n",
    "## PRIMARY KEY (PK) selection explanations: \\\n",
    "## First of all, PK should be unique. UserId, SessionId and ItemInSession can uniquely identify the row in the table, because our query \\\n",
    "## requires information about the current user during current session. \\\n",
    "## We can partition our data by UserId and SessionId which can give us acceptable distribution. Also we need clustering column itemInSession \\\n",
    "## which gurantees us uniqness of data inside session and allows us to order data as we needed.\n",
    "createTableForQuery2 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_sessions(\n",
    "        userId int,\n",
    "        sessionId int,\n",
    "        itemInSession int,\n",
    "        artist text,\n",
    "        song text,\n",
    "        user_first_name text,\n",
    "        user_last_name text,\n",
    "        PRIMARY KEY ((userId, sessionId), itemInSession))\n",
    "        WITH CLUSTERING ORDER BY (itemInSession ASC);\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(createTableForQuery2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "insertDataForQuery2 = \"\"\"\n",
    "    INSERT INTO user_sessions(userId, sessionId, itemInSession, artist, song, user_first_name, user_last_name)\n",
    "    VALUES(%s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader) # skip header\n",
    "        for line in csvreader:\n",
    "            # Choose required fields and convert them to proper datatypes for insertion\n",
    "            session.execute(insertDataForQuery2, (int(line[10]), int(line[8]), int(line[3]), line[0], line[9], line[1], line[4]))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Artist                                               Song  \\\n",
      "0   Down To The Bone                                 Keep On Keepin' On   \n",
      "1       Three Drives                                        Greece 2000   \n",
      "2  Sebastien Tellier                                          Kilometer   \n",
      "3      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n",
      "\n",
      "  First name Last name  \n",
      "0     Sylvie      Cruz  \n",
      "1     Sylvie      Cruz  \n",
      "2     Sylvie      Cruz  \n",
      "3     Sylvie      Cruz  \n"
     ]
    }
   ],
   "source": [
    "## Verify Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "selectForQuery2 = \"\"\"\n",
    "    SELECT artist, song, user_first_name, user_last_name\n",
    "    FROM user_sessions\n",
    "    WHERE userid = 10\n",
    "    AND sessionid = 182;\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(selectForQuery2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "column_labels = ('Artist', 'Song', 'First name', 'Last name')\n",
    "result_df = pd.DataFrame(list(rows), columns=column_labels)\n",
    "print(result_df)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create table for Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "## PRIMARY KEY (PK) selection explanations: \\\n",
    "## There are only fields song, firstName and lastName in the table. In fact this columns does not guarantee the uniqness of the rows \\\n",
    "## in the entre dataset, but for current query we are intrested in the deduplication of data. For example, user \"John Doe\" may listen \\\n",
    "## song 'All Hands Against His Own' multiple times, but we want to output his name once. Apache Cassandra allows us to make this trick \\\n",
    "## just specify only song, firstName and lastName in the PK. \\\n",
    "## Table will be partitioned by the song. It is possible to add some other fields for better distribution but for the current query where we need \\\n",
    "## only unique usernames it is enough. FirstName and lastName will be used as clustering columns.\n",
    "createTableForQuery3 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS song_listeners(\n",
    "        song text,\n",
    "        user_first_name text,\n",
    "        user_last_name text,\n",
    "        PRIMARY KEY (song, user_first_name, user_last_name));\n",
    "\"\"\"\n",
    "try:\n",
    "    session.execute(createTableForQuery3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "insertDataForQuery3 = \"\"\"\n",
    "    INSERT INTO song_listeners(song, user_first_name, user_last_name)\n",
    "    VALUES(%s, %s, %s);\n",
    "\"\"\"\n",
    "    \n",
    "try:\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader) # skip header\n",
    "        for line in csvreader:\n",
    "            # Choose required fields and convert them to proper datatypes for insertion\n",
    "            session.execute(insertDataForQuery3, (line[9], line[1], line[4]))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   First name Last name\n",
      "0  Jacqueline     Lynch\n",
      "1        Sara   Johnson\n",
      "2       Tegan    Levine\n"
     ]
    }
   ],
   "source": [
    "## Verify Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "selectForQuery3 = \"\"\"\n",
    "    SELECT user_first_name, user_last_name\n",
    "    FROM song_listeners\n",
    "    WHERE song = 'All Hands Against His Own';\n",
    "\"\"\"\n",
    "try:\n",
    "    rows = session.execute(selectForQuery3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "column_labels = ('First name', 'Last name')\n",
    "result_df = pd.DataFrame(list(rows), columns=column_labels)\n",
    "print(result_df)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## TO-DO: Drop the table before closing out the sessions\n",
    "dropTableForQuery1 = \"DROP TABLE IF EXISTS music_app_sessions;\"\n",
    "dropTableForQuery2 = \"DROP TABLE IF EXISTS user_sessions;\"\n",
    "dropTableForQuery3 = \"DROP TABLE IF EXISTS song_listeners;\"\n",
    "\n",
    "try:\n",
    "    session.execute(dropTableForQuery1)\n",
    "    session.execute(dropTableForQuery2)\n",
    "    session.execute(dropTableForQuery3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
