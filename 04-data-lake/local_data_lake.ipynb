{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import udf, col, asc, desc, countDistinct\n",
    "from pyspark.sql.functions import date_format, row_number, monotonically_increasing_id \n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "\n",
    "log_files = zf.ZipFile('data/log-data.zip', 'r')\n",
    "log_files.extractall('data/log_data')\n",
    "log_files.close()\n",
    "\n",
    "song_files = zf.ZipFile('data/song-data.zip', 'r')\n",
    "song_files.extractall('data')\n",
    "song_files.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('DataLake') \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logDf = spark.read.json('data/log_data/*')\n",
    "songDf = spark.read.json('data/song_data/*/*/*/*')\n",
    "\n",
    "# logDf = spark \\\n",
    "#     .read \\\n",
    "#     .format('json') \\\n",
    "#     .option('inferSchema', 'true') \\\n",
    "#     .load('data/log_data/*')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songDf.show(1)\n",
    "songDf.count()\n",
    "songDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songs_schema = StructType([\n",
    "    StructField('song_id', StringType(), nullable=False),\n",
    "    StructField('title', StringType(), nullable=False),\n",
    "    StructField('artist_id', StringType(), nullable=True),\n",
    "    StructField('year', LongType(), nullable=True),\n",
    "    StructField('duration', DoubleType(), nullable=True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songs_rdd = songDf \\\n",
    "    .filter(col('song_id').isNotNull()) \\\n",
    "    .filter(col('title').isNotNull()) \\\n",
    "    .select('song_id', 'title', 'artist_id', 'year', 'duration') \\\n",
    "    .rdd\n",
    "\n",
    "songs = spark.createDataFrame(songs_rdd, songs_schema)\n",
    "\n",
    "songs.show(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songs \\\n",
    "    .repartition('year', 'artist_id') \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('tables/songs/songs.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songs_read = spark.read.parquet('tables/songs/songs.parquet')\n",
    "songs_read.createOrReplaceTempView('songs')\n",
    "spark.sql('SELECT * FROM songs WHERE title LIKE \"Law%\"').show()\n",
    "spark.sql('SELECT COUNT(*) FROM songs').show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artists_schema = StructType([\n",
    "    StructField('artist_id', StringType(), nullable=False),\n",
    "    StructField('name', StringType(), nullable=False),\n",
    "    StructField('location', StringType(), nullable=True),\n",
    "    StructField('latitude', DoubleType(), nullable=True),\n",
    "    StructField('longitude', DoubleType(), nullable=True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artists_rdd = songDf \\\n",
    "    .filter(col('artist_id').isNotNull()) \\\n",
    "    .filter(col('artist_name').isNotNull()) \\\n",
    "    .select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude') \\\n",
    "    .rdd\n",
    "\n",
    "artists = spark.createDataFrame(artists_rdd, artists_schema)\n",
    "\n",
    "artists.show(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artists.write.parquet('tables/artists/artists.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logDf.show(1)\n",
    "logDf.count()\n",
    "logDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logDf.count()\n",
    "logDf.groupBy('userId').count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = [\n",
    "    {'name': 'Alice', 'age': 1},\n",
    "    {'name': 'Alice', 'age': 2},\n",
    "    {'name': 'Bob', 'age': 5}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(d)\n",
    "df.show()\n",
    "\n",
    "w = Window.partitionBy('name').orderBy(col('age').desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.withColumn('rn', row_number().over(w)).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users_schema = StructType([\n",
    "    StructField('user_id', LongType(), nullable=False),\n",
    "    StructField('first_name', StringType(), nullable=True),\n",
    "    StructField('last_name', StringType(), nullable=True),\n",
    "    StructField('gender', StringType(), nullable=True),\n",
    "    StructField('level', StringType(), nullable=True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users_window = Window \\\n",
    "    .partitionBy('userId') \\\n",
    "    .orderBy(col('ts').desc()) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "users_rdd = logDf \\\n",
    "    .filter(col('page') == 'NextSong') \\\n",
    "    .filter(col('userId').isNotNull()) \\\n",
    "    .withColumn('num', row_number().over(users_window)) \\\n",
    "    .withColumn('user_id', col('userId').cast(LongType())) \\\n",
    "    .filter(col('num') == 1) \\\n",
    "    .select('user_id', 'firstName', 'lastName', 'gender', 'level') \\\n",
    "    .rdd\n",
    "    \n",
    "users = spark.createDataFrame(users_rdd, users_schema)\n",
    "\n",
    "users.show(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users.write.parquet('tables/users/users.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_schema = StructType([\n",
    "    StructField('start_time', TimestampType(), nullable=False),\n",
    "    StructField('hour', IntegerType(), nullable=False),\n",
    "    StructField('day', IntegerType(), nullable=False),\n",
    "    StructField('week', IntegerType(), nullable=False),\n",
    "    StructField('month', IntegerType(), nullable=False),\n",
    "    StructField('year', IntegerType(), nullable=False),\n",
    "    StructField('weekday', IntegerType(), nullable=False)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logDf.select('ts').show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_rdd = logDf \\\n",
    "    .select('ts') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('timestamp', (col('ts') / 1000).cast(TimestampType())) \\\n",
    "    .select(\n",
    "        col('timestamp').alias('start_time'),\n",
    "        hour('timestamp').alias('hour'),\n",
    "        dayofmonth('timestamp').alias('day'),\n",
    "        weekofyear('timestamp').alias('week'),\n",
    "        month('timestamp').alias('month'),\n",
    "        year('timestamp').alias('year'),\n",
    "        date_format(col('timestamp'), 'F').cast(IntegerType()).alias('weekday')\n",
    "    ) \\\n",
    "    .rdd\n",
    "\n",
    "time = spark.createDataFrame(time_rdd, time_schema)\n",
    "\n",
    "time.show(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time \\\n",
    "    .repartition('year', 'month') \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('tables/time/time.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songplays_schema = StructType([\n",
    "    StructField('songplay_id', LongType(), nullable=False),\n",
    "    StructField('start_time', TimestampType(), nullable=False),\n",
    "    StructField('user_id', LongType(), nullable=False),\n",
    "    StructField('level', StringType(), nullable=True),\n",
    "    StructField('song_id', StringType(), nullable=False),\n",
    "    StructField('artist_id', StringType(), nullable=False),\n",
    "    StructField('session_id', LongType(), nullable=True),\n",
    "    StructField('location', StringType(), nullable=True),\n",
    "    StructField('user_agent', StringType(), nullable=True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean_logDf = logDf \\\n",
    "    .filter(col('page') == 'NextSong')\n",
    "\n",
    "clean_songDf = songDf \\\n",
    "    .filter(col('song_id').isNotNull()) \\\n",
    "    .filter(col('artist_id').isNotNull())\n",
    "\n",
    "songplays_rdd = clean_songDf \\\n",
    "    .join(clean_logDf,\n",
    "        (clean_songDf.title == clean_logDf.song)\n",
    "            & (clean_songDf.artist_name == clean_logDf.artist)\n",
    "            & (clean_songDf.duration == clean_logDf.length)\n",
    "        , 'inner') \\\n",
    "    .withColumn('id', monotonically_increasing_id() + 1) \\\n",
    "    .withColumn('start_time', (col('ts') / 1000).cast(TimestampType())) \\\n",
    "    .withColumn('user_id', col('userId').cast(LongType())) \\\n",
    "    .select('id', 'start_time', 'user_id', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent') \\\n",
    "    .rdd\n",
    "\n",
    "\n",
    "songplays = spark.createDataFrame(songplays_rdd, songplays_schema)\n",
    "\n",
    "songplays.show(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "songplays \\\n",
    "    .repartition(year('start_time'), month('start_time')) \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('tables/songplays/songplays.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
