{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Building Data Lake for demographic analysis and prediction in the U.S.\n",
    "\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "Main goal of the project is to building Data Lake for demographic analysis and prediction in the U.S.\n",
    "\n",
    "There are several datasets with raw data:\n",
    "\n",
    "* `I94 Immigration Data`;\n",
    "* `U.S. City Demographic Data`;\n",
    "* `Airport Codes`.\n",
    "\n",
    "Project focuses on creating data model for the future analysis and building ETL-pipeline using Apache Spark. The whole ETL-pipeline processes raw data, cleanup, apply data quality checks and store to the analytical area in the Data Lake according to data model.\n",
    "Resulting database allows analytics and data scientists to find and predict correlation between demographic situation in different cities/states and immigration to this locations.\n",
    "\n",
    "The project follows the follow steps:\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set pandas options\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create and configure Spark session. Add support for sas7bdat data format.\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config('spark.jars.packages','saurfang:spark-sas7bdat:2.0.0-s_2.11')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "\n",
    "We will build a database (in the analytical area of our data lake) for demographic analysis and predictions which depends on immegration data.\n",
    "Our resulting database will have the following structure:\n",
    "\n",
    "* `airports` – dimension table which contains data about U.S. international airports. This table filled from the raw datasets represented by `I94 Immigration Data` SAS database joined with [airport-codes_csv.csv](./airport-codes_csv.csv) data source files. This table is not necessary for the solution, but `I94 Immigration Data` has own airport identifiers and we need to provide standard airport [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) codes for our analysis team.\n",
    "* `cities` – dimension table with U.S. cities that will interested us in the future analytics. This table filled from the raw dataset represented by [us-cities-demographics.json](./us-cities-demographics.json) datasource file. We want to analyze only data for the same cities to compare with aggregated data for the 2015 year.\n",
    "* `immigration` – fact table which contains aggregated data grouped by year, month, and city. Each row of this table shows how much people (men, women and total) immgegrated to the particular U.S. city each month. This table filled from the raw dataset represented by `I94 Immigration Data` SAS database joined with `cities` data.\n",
    "* `demographics` – pre-aggregated materialized view which is good candidate to land in data mart and contains aggregated data grouped by year and city. It can be used for the fast analytics and reporting. This table filled in two steps:\n",
    "  1. Filled from the `us-cities-demographics.csv` datasource file which already contains data for the 2015 year.\n",
    "  2. Filled from the `immigration` fact table by additition to the data for the previous years which allows us to see growth of the population.\n",
    "\n",
    "##### Important notes and assumptions\n",
    "\n",
    "1. We assume that all immigrants from the `I94 Immigration Data` will come and stay in the U.S. Actually we could analyze their visa types to be more precises, but for the scope of this project we will not consider it.\n",
    "2. We assume that all immigrants will stay for living in the city to which the airport belongs. In real life this may not be the case, but we will not consider it.\n",
    "3. Actually `demographics` table will contains data only for the 2015 and 2016 years because the `I94 Immigration Data` dataset contains data only for the 2016 year but our pipeliens will work for datasets with following years too.\n",
    "\n",
    "##### Tools\n",
    "\n",
    "We will use [Apache Spark](https://spark.apache.org/) to build our pipeline. Also we will use Data Lake idea to build our resulting tables and store our data in the HDFS or object storage like S3. We will not create staging area because in the Data Lake approache we can just load raw datasets with Spark, make all necessary transformations, cleanup the data, apply data quality checks and land it in the analytical area.\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "We will use several data sources to achieve our goal.\n",
    "\n",
    "1. We will use `U.S. City Demographic Data` grouped by city for 2015 year provided by [U.S. Census Bureau](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/).\n",
    "  * Data format: [JSON](https://en.wikipedia.org/wiki/JSON). You may see both JSON and CSV files with the same dataset in the repository, but we choose JSON format because we want to use different formats for each dataset.\n",
    "  * Data source: [us-cities-demographics.json](./us-cities-demographics.json)\n",
    "2. We will extract U.S. immigration data grouped by year, month and city from `I94 Immigration Data` provided by [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "  * Data format: [SAS7BDAT](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf)\n",
    "  * Data format description: SAS labels and possible values and errors described in the [I94_SAS_Labels_Descriptions.SAS](./I94_SAS_Labels_Descriptions.SAS) file.\n",
    "  * Data source: [i94_apr16_sub.sas7bdat](../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat). Actually there are bunch of files partitioned by month.\n",
    "3. Also we will use [Airport Codes](https://datahub.io/core/airport-codes#data) table.\n",
    "  * Data format: [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)\n",
    "  * Data source: [airport-codes_csv.csv](./airport-codes_csv.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "As you may see in the example below demogrphic data stored in the `fields` column. There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* city;\n",
    "* state;\n",
    "* state_code;\n",
    "* male_population;\n",
    "* female_population;\n",
    "* total_population;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetid</th>\n",
       "      <th>fields</th>\n",
       "      <th>record_timestamp</th>\n",
       "      <th>recordid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 2177650, 'city': 'Los Angeles', 'number_of_veterans': 85417, 'male_population': 1958998, 'foreign_born': 1485425, 'average_household_size': 2.86, 'median_age': 35.0, 'state': 'California', 'race': 'White', 'total_population': 3971896, 'state_code': 'CA', 'female_population': 2012898}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>7da42fda61238faccac3d43954a8f621a3a51194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 124270, 'city': 'Metairie', 'number_of_veterans': 7187, 'male_population': 69515, 'foreign_born': 19871, 'average_household_size': 2.39, 'median_age': 41.6, 'state': 'Louisiana', 'race': 'White', 'total_population': 146458, 'state_code': 'LA', 'female_population': 76943}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>f176fd28e69ee0c152db080858781e769840c6cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 80781, 'city': 'Boca Raton', 'number_of_veterans': 4367, 'male_population': 44760, 'foreign_born': 21117, 'average_household_size': 2.22, 'median_age': 47.3, 'state': 'Florida', 'race': 'White', 'total_population': 93226, 'state_code': 'FL', 'female_population': 48466}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>ed4a94bf11b553ed8ebd93b7c24b0e8f326dadb0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 2566, 'city': 'Quincy', 'number_of_veterans': 4147, 'male_population': 44129, 'foreign_born': 32935, 'average_household_size': 2.39, 'median_age': 41.0, 'state': 'Massachusetts', 'race': 'Hispanic or Latino', 'total_population': 93629, 'state_code': 'MA', 'female_population': 49500}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>a4cd5f6782c79aa0348652718ca179ca390ce086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 16845, 'city': 'Union City', 'number_of_veterans': 1440, 'male_population': 38599, 'foreign_born': 32752, 'average_household_size': 3.46, 'median_age': 38.5, 'state': 'California', 'race': 'White', 'total_population': 74510, 'state_code': 'CA', 'female_population': 35911}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>b3b36950de605af92a78168f5127dff485a59b0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                datasetid                                                                                                                                                                                                                                                                                                  fields           record_timestamp                                  recordid\n",
       "0  us-cities-demographics  {'count': 2177650, 'city': 'Los Angeles', 'number_of_veterans': 85417, 'male_population': 1958998, 'foreign_born': 1485425, 'average_household_size': 2.86, 'median_age': 35.0, 'state': 'California', 'race': 'White', 'total_population': 3971896, 'state_code': 'CA', 'female_population': 2012898}  1970-01-01T03:00:00+03:00  7da42fda61238faccac3d43954a8f621a3a51194\n",
       "1  us-cities-demographics               {'count': 124270, 'city': 'Metairie', 'number_of_veterans': 7187, 'male_population': 69515, 'foreign_born': 19871, 'average_household_size': 2.39, 'median_age': 41.6, 'state': 'Louisiana', 'race': 'White', 'total_population': 146458, 'state_code': 'LA', 'female_population': 76943}  1970-01-01T03:00:00+03:00  f176fd28e69ee0c152db080858781e769840c6cc\n",
       "2  us-cities-demographics                 {'count': 80781, 'city': 'Boca Raton', 'number_of_veterans': 4367, 'male_population': 44760, 'foreign_born': 21117, 'average_household_size': 2.22, 'median_age': 47.3, 'state': 'Florida', 'race': 'White', 'total_population': 93226, 'state_code': 'FL', 'female_population': 48466}  1970-01-01T03:00:00+03:00  ed4a94bf11b553ed8ebd93b7c24b0e8f326dadb0\n",
       "3  us-cities-demographics   {'count': 2566, 'city': 'Quincy', 'number_of_veterans': 4147, 'male_population': 44129, 'foreign_born': 32935, 'average_household_size': 2.39, 'median_age': 41.0, 'state': 'Massachusetts', 'race': 'Hispanic or Latino', 'total_population': 93629, 'state_code': 'MA', 'female_population': 49500}  1970-01-01T03:00:00+03:00  a4cd5f6782c79aa0348652718ca179ca390ce086\n",
       "4  us-cities-demographics              {'count': 16845, 'city': 'Union City', 'number_of_veterans': 1440, 'male_population': 38599, 'foreign_born': 32752, 'average_household_size': 3.46, 'median_age': 38.5, 'state': 'California', 'race': 'White', 'total_population': 74510, 'state_code': 'CA', 'female_population': 35911}  1970-01-01T03:00:00+03:00  b3b36950de605af92a78168f5127dff485a59b0b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the U.S. City Demographic Data and display first 5 rows\n",
    "demographics_source_file_name = './us-cities-demographics.json'\n",
    "demographics_demo_df = pd.read_json(demographics_source_file_name)\n",
    "demographics_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* `cicid` – unique row identifier;\n",
    "* `i94yr` – year in 4 digit format;\n",
    "* `i94mon` – month index (Jan = 1, Feb = 2, Mar = 3, etc.);\n",
    "* `i94port` – airport code in 3 letters format described in the SAS file description;\n",
    "* `i94mode` – border crossing method (Air = 1, Sea = 2, Land = 3, Not reported = 9);\n",
    "* `i94addr` – U.S. state of arrival;\n",
    "* `gender` – male (M) or female (F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum airline        admnum  fltno visatype\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09    NaN       B2\n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL      NaN   ...           Y      NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09  00296       F1\n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI  20691.0   ...         NaN        M   1961.0  09302016      M    NaN      OS  6.666432e+08     93       B2\n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2\n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2\n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the I94 Immigration Data and display first 5 rows\n",
    "immigration_source_file_name = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_demo_df = pd.read_sas(immigration_source_file_name, 'sas7bdat', encoding='ISO-8859-1')\n",
    "immigration_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Codes\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* `iata_code` – unique [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) airport code, we will extract only international airports and can use this code as identifier;\n",
    "* `name` – airport name;\n",
    "* `iso_country` – country, we will use this field to extract only U.S. airports;\n",
    "* `iso_region` – country + state, we will use it to extract state;\n",
    "* `municipality` – city to which the airport belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft continent iso_country iso_region  municipality gps_code iata_code local_code                            coordinates\n",
       "0   00A       heliport                   Total Rf Heliport          11.0       NaN          US      US-PA      Bensalem      00A       NaN        00A     -74.93360137939453, 40.07080078125\n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0       NaN          US      US-KS         Leoti     00AA       NaN       00AA                 -101.473911, 38.704022\n",
       "2  00AK  small_airport                        Lowell Field         450.0       NaN          US      US-AK  Anchor Point     00AK       NaN       00AK            -151.695999146, 59.94919968\n",
       "3  00AL  small_airport                        Epps Airpark         820.0       NaN          US      US-AL       Harvest     00AL       NaN       00AL  -86.77030181884766, 34.86479949951172\n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0       NaN          US      US-AR       Newport      NaN       NaN        NaN                    -91.254898, 35.6087"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the Airport Codes and display first 5 rows\n",
    "airports_source_file_name = './airport-codes_csv.csv'\n",
    "airports_demo_df = pd.read_csv(airports_source_file_name, sep=',')\n",
    "airports_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Explore the Data\n",
    "\n",
    "For each of our data sources we will provide it's own cleaning steps.\n",
    "Important part of this step that we just cleanup and filter raw data sets, we do not perform any joins between our data frames yet. It is preparation step to build our data model in following steps.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "1. Load and cleanup `U.S. City Demographic Data`\n",
    "2. Load and cleanup `I94 Immigration Data`\n",
    "  * Parse `I94_SAS_Labels_Descriptions.SAS`\n",
    "  * Cleaup `I94 Immigration Data`\n",
    "3. Load and cleanup `Airport Codes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load and cleanup U.S. City Demographic Data\n",
    "\n",
    "Fortunately U.S. City Demographic dataset is pretty good and do not require any cleaning steps. But the essential data lives in the nested object, so we need extract it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "|average_household_size|       city|  count|female_population|foreign_born|male_population|median_age|number_of_veterans|              race|        state|state_code|total_population|\n",
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "|                  2.86|Los Angeles|2177650|          2012898|     1485425|        1958998|      35.0|             85417|             White|   California|        CA|         3971896|\n",
      "|                  2.39|   Metairie| 124270|            76943|       19871|          69515|      41.6|              7187|             White|    Louisiana|        LA|          146458|\n",
      "|                  2.22| Boca Raton|  80781|            48466|       21117|          44760|      47.3|              4367|             White|      Florida|        FL|           93226|\n",
      "|                  2.39|     Quincy|   2566|            49500|       32935|          44129|      41.0|              4147|Hispanic or Latino|Massachusetts|        MA|           93629|\n",
      "|                  3.46| Union City|  16845|            35911|       32752|          38599|      38.5|              1440|             White|   California|        CA|           74510|\n",
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_demographic(source_filename):\n",
    "    \"\"\"\n",
    "        Loads demographic data from the source file in JSON format\n",
    "        and extract data from the inner JSON property.\n",
    "        It also should apply cleanup steps, but the existing dataset\n",
    "        already good quality.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load raw dataset as JSON using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('json')\n",
    "        .load(source_filename)\n",
    "    )\n",
    "\n",
    "    # Actual data lives in the `fields` property, so we have to extract it.\n",
    "    cleaned_df = raw_df.select('fields.*')\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_demographic function\n",
    "# demographic_df = load_and_cleanup_demographic('./us-cities-demographics.json')\n",
    "# demographic_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and cleanup I94 Immigration Data\n",
    "\n",
    "1. We have a lot of work to cleanup `I94 Immigration Data`. It is stored in SAS format and there is also exists `I94_SAS_Labels_Descriptions.SAS` file with description how to parse dataset values.\n",
    "We need to write helper function which can map dataset columns values to real values using this description. This is not a cleanup step, but it is important preparation step for the future work.\n",
    "\n",
    "2. Also we have to cleanup entire `I94 Immigration Data` dataset:\n",
    "\n",
    "  * `i94port` should be in the list of valid airports described in the `I94_SAS_Labels_Descriptions.SAS` file;\n",
    "  * `i94mode` = 1, we are interested in only in people who arrived to the U.S. by air, because only for them we can determine destination city;\n",
    "  * `gender` is not empty, because we want to calculate statistics by men and women separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I94 Airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALC</th>\n",
       "      <td>ALCAN, AK [ALC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANC</th>\n",
       "      <td>ANCHORAGE, AK [ANC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAR</th>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK [BAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAC</th>\n",
       "      <td>DALTONS CACHE, AK [DAC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIZ</th>\n",
       "      <td>DEW STATION PT LAY DEW, AK [PIZ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            I94 Airport\n",
       "ALC                     ALCAN, AK [ALC]\n",
       "ANC                 ANCHORAGE, AK [ANC]\n",
       "BAR  BAKER AAF - BAKER ISLAND, AK [BAR]\n",
       "DAC             DALTONS CACHE, AK [DAC]\n",
       "PIZ    DEW STATION PT LAY DEW, AK [PIZ]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class I94Airport:\n",
    "    \"\"\"\n",
    "        I94Airport contains parsed information from `i94port` field\n",
    "        of the `I94 Immigration Data`.\n",
    "        Also it implements static method `sas_description_parse_airport_codes`\n",
    "        which parses SAS descriptions and returns dictionary\n",
    "        with airport codes as keys and I94Aiport objects as values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, airport_code, us_city, us_state):\n",
    "        self._airport_code = airport_code\n",
    "        self._us_city = us_city\n",
    "        self._us_state = us_state\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self._us_city}, {self._us_state} [{self._airport_code}]'\n",
    "\n",
    "    @property\n",
    "    def airport_code(self):\n",
    "        return self._airport_code\n",
    "\n",
    "    @property\n",
    "    def us_city(self):\n",
    "        return self._us_city\n",
    "\n",
    "    @property\n",
    "    def us_state(self):\n",
    "        return self._us_state\n",
    "\n",
    "    @staticmethod\n",
    "    def sas_description_parse_airport_codes(sas_description_filename):\n",
    "        \"\"\"\n",
    "            Load SAS description file, parse `i94port` values,\n",
    "            filter non-U.S., \"Collapsed\" or \"No port\" data\n",
    "            and returns dictionary with parsed results to fast lookups.\n",
    "        \"\"\"\n",
    "\n",
    "        airports = {}\n",
    "        is_airports_section = False\n",
    "\n",
    "        # We will use regex to extract all three parts (code, city and state) from the description file.\n",
    "        # Example: 'LOS' = 'LOS ANGELES, CA       '\n",
    "        # Regex pattern matching: LOS = match[0], LOS ANGELES = match[1], CA = match[2]\n",
    "        # Also current structure of regex eliminates all invalid airport codes.\n",
    "        airport_regex = re.compile(r'\\'([A-Z0-9]{3})\\'.*=.*\\'([A-Z-\\/,\\.\\ ]+),\\ ?([A-Z]{2})\\ *\\'')\n",
    "\n",
    "        with open(sas_description_filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == ';':\n",
    "                    is_airports_section = False\n",
    "                elif line == 'value $i94prtl':\n",
    "                    is_airports_section = True\n",
    "                elif is_airports_section:\n",
    "                    airport_parts = airport_regex.findall(line)\n",
    "                    if airport_parts:\n",
    "                        airport_code, us_city, us_state = airport_parts[0]\n",
    "                        airports[airport_code] = I94Airport(airport_code, us_city, us_state)\n",
    "        return airports\n",
    "\n",
    "\n",
    "# Uncomment following code to test class I94Airport and it's static method sas_description_parse_airport_codes\n",
    "# i94_airports = I94Airport.sas_description_parse_airport_codes('./I94_SAS_Labels_Descriptions.SAS')\n",
    "# i94_airports_df = pd.DataFrame.from_dict(i94_airports, orient='index', columns=['I94 Airport'])\n",
    "# i94_airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|20549.0|  58.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1958.0|04062016|     M|  null|     LH|9.247876383E10|00422|      B1|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20549.0|  56.0|    1.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1960.0|04062016|     F|  null|     LH|9.247890033E10|00422|      B1|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|20561.0|  62.0|    2.0|  1.0|20160401|     TIA| null|      G|      O|   null|      M| 1954.0|09302016|     M|  null|     AZ|9.250378143E10|00614|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_i94_immigration(sas_source_filename, sas_description_filename):\n",
    "    \"\"\"\n",
    "        Loads and cleanup I94 Immigration Data.\n",
    "        Apply cleanup steps:\n",
    "        - i94port should be in the list of valid airports;\n",
    "        - i94mode = 1 (arrived to the U.S. by air);\n",
    "        - gender is not empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load airports from SAS description file\n",
    "    i94_airports = I94Airport.sas_description_parse_airport_codes(sas_description_filename)\n",
    "\n",
    "    # Load raw dataset as SAS using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('com.github.saurfang.sas.spark')\n",
    "        .load(sas_source_filename)\n",
    "    )\n",
    "\n",
    "    # Cleanup data\n",
    "    cleaned_df = (\n",
    "        raw_df\n",
    "        .filter(F.col('i94mode') == 1)\n",
    "        .filter(F.col('gender').isNotNull())\n",
    "        .filter(F.col('i94port').isin(list(i94_airports.keys())))\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_i94_immigration function\n",
    "# immigration_df = load_and_cleanup_i94_immigration('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', './I94_SAS_Labels_Descriptions.SAS')\n",
    "# immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load and cleanup Airport Codes\n",
    "\n",
    "We are interested only in the U.S. airports, thus we will preserve data by a condition `iso_country` = `US`.\n",
    "Also we need only international airports, thus we will filter out all data with empty `iata_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_airport_codes(source_filename):\n",
    "    \"\"\"\n",
    "        Loads airport codes data from the source file in CSV format.\n",
    "        Apply cleanup steps:\n",
    "        - preserve only U.S. airport codes;\n",
    "        - remove all rows with empty IATA code.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load raw dataset as CSV using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('csv')\n",
    "        .option('header', 'true')\n",
    "        .option('delimiter', ',')\n",
    "        .load(source_filename)\n",
    "    )\n",
    "\n",
    "    # Cleanup data\n",
    "    cleaned_df = (\n",
    "        raw_df\n",
    "        .filter(F.col('iso_country') == 'US')\n",
    "        .filter(F.col('iata_code').isNotNull())\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_demographic function\n",
    "# airport_codes_df = load_and_cleanup_airport_codes('./airport-codes_csv.csv')\n",
    "# airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data\n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}