{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Building Data Lake for demographic analysis and prediction in the U.S.\n",
    "\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "Main goal of the project is to building Data Lake for demographic analysis and prediction in the U.S.\n",
    "\n",
    "There are several datasets with raw data:\n",
    "\n",
    "* `I94 Immigration Data`;\n",
    "* `U.S. City Demographic Data`;\n",
    "* `Airport Codes`.\n",
    "\n",
    "Project focuses on creating data model for the future analysis and building ETL-pipeline using Apache Spark. The whole ETL-pipeline processes raw data, cleanup, apply data quality checks and store to the analytical area in the Data Lake according to data model.\n",
    "Resulting database allows analytics and data scientists to find and predict correlation between demographic situation in different cities/states and immigration to this locations.\n",
    "\n",
    "The project follows the follow steps:\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set pandas options\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create and configure Spark session. Add support for sas7bdat data format.\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config('spark.jars.packages','saurfang:spark-sas7bdat:2.0.0-s_2.11')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "\n",
    "We will build a database (in the analytical area of our data lake) for demographic analysis and predictions which depends on immegration data.\n",
    "Our resulting database will have the following structure:\n",
    "\n",
    "* `cities` – dimension table with U.S. cities that will interested us in the future analytics. This table filled from the raw dataset represented by [us-cities-demographics.json](./us-cities-demographics.json) datasource file. We want to analyze only data for the same cities to compare with aggregated data for the 2015 year.\n",
    "* `airports` – dimension table which contains data about U.S. international airports. This table filled from the raw dataset represented by [airport-codes_csv.csv](./airport-codes_csv.csv) data source file enriched with airport code in I94 data format (see explanation in the step 3). This table is not necessary for the solution, but `I94 Immigration Data` has own airport identifiers and we need to provide standard airport [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) codes for our analysis team.\n",
    "* `immigration` – fact table register immigration events.This table filled from the raw dataset represented by `I94 Immigration Data` SAS database joined with `airports` and `cities` data.\n",
    "* `demographics` – pre-aggregated materialized view which is good candidate to land in data mart and contains aggregated data grouped by year and city. Each row of this table shows how much people (men, women and total) immigrated to the particular U.S. city each year. It can be used for the fast analytics and reporting. This table filled in two steps:\n",
    "  1. Filled from the `us-cities-demographics.csv` datasource file which already contains data for the 2015 year.\n",
    "  2. Filled from the `immigration` fact table by additition to the data for the previous years which allows us to see growth of the population.\n",
    "\n",
    "##### Important notes and assumptions\n",
    "\n",
    "1. We assume that all immigrants from the `I94 Immigration Data` will come and stay in the U.S. Actually we could analyze their visa types to be more precises, but for the scope of this project we will not consider it.\n",
    "2. We assume that all immigrants will stay for living in the city to which the airport belongs. In real life this may not be the case, but we will not consider it.\n",
    "3. Actually `demographics` table will contains data only for the 2015 and 2016 years because the `I94 Immigration Data` dataset contains data only for the 2016 year but our pipeliens will work for datasets with following years too.\n",
    "\n",
    "##### Tools\n",
    "\n",
    "We will use [Apache Spark](https://spark.apache.org/) to build our pipeline. Also we will use Data Lake idea to build our resulting tables and store our data in the HDFS or object storage like S3. We will not create staging area because in the Data Lake approache we can just load raw datasets with Spark, make all necessary transformations, cleanup the data, apply data quality checks and land it in the analytical area.\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "We will use several data sources to achieve our goal.\n",
    "\n",
    "1. We will use `U.S. City Demographic Data` grouped by city for 2015 year provided by [U.S. Census Bureau](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/).\n",
    "  * Data format: [JSON](https://en.wikipedia.org/wiki/JSON). You may see both JSON and CSV files with the same dataset in the repository, but we choose JSON format because we want to use different formats for each dataset.\n",
    "  * Data source: [us-cities-demographics.json](./us-cities-demographics.json)\n",
    "2. We will extract U.S. immigration data grouped by year, month and city from `I94 Immigration Data` provided by [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "  * Data format: [SAS7BDAT](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf)\n",
    "  * Data format description: SAS labels and possible values and errors described in the [I94_SAS_Labels_Descriptions.SAS](./I94_SAS_Labels_Descriptions.SAS) file.\n",
    "  * Data source: [i94_apr16_sub.sas7bdat](../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat). Actually there are bunch of files partitioned by month.\n",
    "3. Also we will use [Airport Codes](https://datahub.io/core/airport-codes#data) table.\n",
    "  * Data format: [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)\n",
    "  * Data source: [airport-codes_csv.csv](./airport-codes_csv.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "As you may see in the example below demogrphic data stored in the `fields` column. There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* city;\n",
    "* state;\n",
    "* state_code;\n",
    "* male_population;\n",
    "* female_population;\n",
    "* total_population;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetid</th>\n",
       "      <th>fields</th>\n",
       "      <th>record_timestamp</th>\n",
       "      <th>recordid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 2177650, 'city': 'Los Angeles', 'number_of_veterans': 85417, 'male_population': 1958998, 'foreign_born': 1485425, 'average_household_size': 2.86, 'median_age': 35.0, 'state': 'California', 'race': 'White', 'total_population': 3971896, 'state_code': 'CA', 'female_population': 2012898}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>7da42fda61238faccac3d43954a8f621a3a51194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 124270, 'city': 'Metairie', 'number_of_veterans': 7187, 'male_population': 69515, 'foreign_born': 19871, 'average_household_size': 2.39, 'median_age': 41.6, 'state': 'Louisiana', 'race': 'White', 'total_population': 146458, 'state_code': 'LA', 'female_population': 76943}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>f176fd28e69ee0c152db080858781e769840c6cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 80781, 'city': 'Boca Raton', 'number_of_veterans': 4367, 'male_population': 44760, 'foreign_born': 21117, 'average_household_size': 2.22, 'median_age': 47.3, 'state': 'Florida', 'race': 'White', 'total_population': 93226, 'state_code': 'FL', 'female_population': 48466}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>ed4a94bf11b553ed8ebd93b7c24b0e8f326dadb0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 2566, 'city': 'Quincy', 'number_of_veterans': 4147, 'male_population': 44129, 'foreign_born': 32935, 'average_household_size': 2.39, 'median_age': 41.0, 'state': 'Massachusetts', 'race': 'Hispanic or Latino', 'total_population': 93629, 'state_code': 'MA', 'female_population': 49500}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>a4cd5f6782c79aa0348652718ca179ca390ce086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>{'count': 16845, 'city': 'Union City', 'number_of_veterans': 1440, 'male_population': 38599, 'foreign_born': 32752, 'average_household_size': 3.46, 'median_age': 38.5, 'state': 'California', 'race': 'White', 'total_population': 74510, 'state_code': 'CA', 'female_population': 35911}</td>\n",
       "      <td>1970-01-01T03:00:00+03:00</td>\n",
       "      <td>b3b36950de605af92a78168f5127dff485a59b0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                datasetid                                                                                                                                                                                                                                                                                                  fields           record_timestamp                                  recordid\n",
       "0  us-cities-demographics  {'count': 2177650, 'city': 'Los Angeles', 'number_of_veterans': 85417, 'male_population': 1958998, 'foreign_born': 1485425, 'average_household_size': 2.86, 'median_age': 35.0, 'state': 'California', 'race': 'White', 'total_population': 3971896, 'state_code': 'CA', 'female_population': 2012898}  1970-01-01T03:00:00+03:00  7da42fda61238faccac3d43954a8f621a3a51194\n",
       "1  us-cities-demographics               {'count': 124270, 'city': 'Metairie', 'number_of_veterans': 7187, 'male_population': 69515, 'foreign_born': 19871, 'average_household_size': 2.39, 'median_age': 41.6, 'state': 'Louisiana', 'race': 'White', 'total_population': 146458, 'state_code': 'LA', 'female_population': 76943}  1970-01-01T03:00:00+03:00  f176fd28e69ee0c152db080858781e769840c6cc\n",
       "2  us-cities-demographics                 {'count': 80781, 'city': 'Boca Raton', 'number_of_veterans': 4367, 'male_population': 44760, 'foreign_born': 21117, 'average_household_size': 2.22, 'median_age': 47.3, 'state': 'Florida', 'race': 'White', 'total_population': 93226, 'state_code': 'FL', 'female_population': 48466}  1970-01-01T03:00:00+03:00  ed4a94bf11b553ed8ebd93b7c24b0e8f326dadb0\n",
       "3  us-cities-demographics   {'count': 2566, 'city': 'Quincy', 'number_of_veterans': 4147, 'male_population': 44129, 'foreign_born': 32935, 'average_household_size': 2.39, 'median_age': 41.0, 'state': 'Massachusetts', 'race': 'Hispanic or Latino', 'total_population': 93629, 'state_code': 'MA', 'female_population': 49500}  1970-01-01T03:00:00+03:00  a4cd5f6782c79aa0348652718ca179ca390ce086\n",
       "4  us-cities-demographics              {'count': 16845, 'city': 'Union City', 'number_of_veterans': 1440, 'male_population': 38599, 'foreign_born': 32752, 'average_household_size': 3.46, 'median_age': 38.5, 'state': 'California', 'race': 'White', 'total_population': 74510, 'state_code': 'CA', 'female_population': 35911}  1970-01-01T03:00:00+03:00  b3b36950de605af92a78168f5127dff485a59b0b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the U.S. City Demographic Data and display first 5 rows\n",
    "demographics_source_file_name = './us-cities-demographics.json'\n",
    "demographics_demo_df = pd.read_json(demographics_source_file_name)\n",
    "demographics_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* `cicid` – unique row identifier;\n",
    "* `i94yr` – year in 4 digit format;\n",
    "* `i94mon` – month index (Jan = 1, Feb = 2, Mar = 3, etc.);\n",
    "* `i94port` – airport code in 3 letters format described in the SAS file description;\n",
    "* `i94mode` – border crossing method (Air = 1, Sea = 2, Land = 3, Not reported = 9);\n",
    "* `i94addr` – U.S. state of arrival;\n",
    "* `gender` – male (M) or female (F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum airline        admnum  fltno visatype\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09    NaN       B2\n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL      NaN   ...           Y      NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09  00296       F1\n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI  20691.0   ...         NaN        M   1961.0  09302016      M    NaN      OS  6.666432e+08     93       B2\n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2\n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN      AA  9.246846e+10  00199       B2\n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the I94 Immigration Data and display first 5 rows\n",
    "immigration_source_file_name = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_demo_df = pd.read_sas(immigration_source_file_name, 'sas7bdat', encoding='ISO-8859-1')\n",
    "immigration_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Codes\n",
    "\n",
    "Extract raw data from the data source and display few rows.\n",
    "\n",
    "There are several fields which we have to extract to achieve our goal:\n",
    "\n",
    "* `iata_code` – unique [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) airport code, we will extract only international airports and can use this code as identifier;\n",
    "* `name` – airport name;\n",
    "* `iso_country` – country, we will use this field to extract only U.S. airports;\n",
    "* `iso_region` – country + state, we will use it to extract state;\n",
    "* `municipality` – city to which the airport belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft continent iso_country iso_region  municipality gps_code iata_code local_code                            coordinates\n",
       "0   00A       heliport                   Total Rf Heliport          11.0       NaN          US      US-PA      Bensalem      00A       NaN        00A     -74.93360137939453, 40.07080078125\n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0       NaN          US      US-KS         Leoti     00AA       NaN       00AA                 -101.473911, 38.704022\n",
       "2  00AK  small_airport                        Lowell Field         450.0       NaN          US      US-AK  Anchor Point     00AK       NaN       00AK            -151.695999146, 59.94919968\n",
       "3  00AL  small_airport                        Epps Airpark         820.0       NaN          US      US-AL       Harvest     00AL       NaN       00AL  -86.77030181884766, 34.86479949951172\n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0       NaN          US      US-AR       Newport      NaN       NaN        NaN                    -91.254898, 35.6087"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data from the Airport Codes and display first 5 rows\n",
    "airports_source_file_name = './airport-codes_csv.csv'\n",
    "airports_demo_df = pd.read_csv(airports_source_file_name, sep=',')\n",
    "airports_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Explore the Data\n",
    "\n",
    "For each of our data sources we will provide it's own cleaning steps.\n",
    "Important part of this step that we just cleanup and filter raw data sets, we do not perform any joins between our data frames yet. It is preparation step to build our data model in following steps.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "1. Load and cleanup `U.S. City Demographic Data`\n",
    "2. Load and cleanup `I94 Immigration Data`\n",
    "  * Parse `I94_SAS_Labels_Descriptions.SAS`\n",
    "  * Cleaup `I94 Immigration Data`\n",
    "3. Load and cleanup `Airport Codes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and cleanup U.S. City Demographic Data\n",
    "\n",
    "`U.S. City Demographic Data` dataset is in JSON format. The essential data lives in the nested object, so we need extract it first.\n",
    "Also we have to check that data in `male_population`, `fenale_population` and `total_population` columns are not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "|average_household_size|       city|  count|female_population|foreign_born|male_population|median_age|number_of_veterans|              race|        state|state_code|total_population|\n",
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "|                  2.86|Los Angeles|2177650|          2012898|     1485425|        1958998|      35.0|             85417|             White|   California|        CA|         3971896|\n",
      "|                  2.39|   Metairie| 124270|            76943|       19871|          69515|      41.6|              7187|             White|    Louisiana|        LA|          146458|\n",
      "|                  2.22| Boca Raton|  80781|            48466|       21117|          44760|      47.3|              4367|             White|      Florida|        FL|           93226|\n",
      "|                  2.39|     Quincy|   2566|            49500|       32935|          44129|      41.0|              4147|Hispanic or Latino|Massachusetts|        MA|           93629|\n",
      "|                  3.46| Union City|  16845|            35911|       32752|          38599|      38.5|              1440|             White|   California|        CA|           74510|\n",
      "+----------------------+-----------+-------+-----------------+------------+---------------+----------+------------------+------------------+-------------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_demographic(source_filename):\n",
    "    \"\"\"\n",
    "        Loads demographic data from the source file in JSON format\n",
    "        and extract data from the inner JSON property.\n",
    "        Apply cleanup steps:\n",
    "        - male population is not empty;\n",
    "        - female population is not empty;\n",
    "        - total population is not empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load raw dataset as JSON using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('json')\n",
    "        .load(source_filename)\n",
    "    )\n",
    "\n",
    "    # Actual data lives in the `fields` property, so we have to extract it.\n",
    "    # Then apply cleanup steps.\n",
    "    cleaned_df = (\n",
    "        raw_df\n",
    "        .select('fields.*')\n",
    "        .filter(F.col('male_population').isNotNull())\n",
    "        .filter(F.col('female_population').isNotNull())\n",
    "        .filter(F.col('total_population').isNotNull())\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_demographic function\n",
    "# demographic_df = load_and_cleanup_demographic('./us-cities-demographics.json')\n",
    "# demographic_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and cleanup I94 Immigration Data\n",
    "\n",
    "1. We have a lot of work to cleanup `I94 Immigration Data`. It is stored in SAS format and there is also exists `I94_SAS_Labels_Descriptions.SAS` file with description how to parse dataset values.\n",
    "We need to write helper function which can map dataset columns values to real values using this description. This is not a cleanup step, but it is important preparation step for the future work.\n",
    "\n",
    "2. Also we have to cleanup entire `I94 Immigration Data` dataset:\n",
    "\n",
    "  * `i94port` should be in the list of valid airports described in the `I94_SAS_Labels_Descriptions.SAS` file;\n",
    "  * `i94mode` = 1, we are interested in only in people who arrived to the U.S. by air, because only for them we can determine destination city;\n",
    "  * `gender` is not empty, because we want to calculate statistics by men and women separately;\n",
    "  * we should exract U.S. city and U.S. state from the `i94port` using SAS descriptions metadata;\n",
    "  * also we should apply data type casts for some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I94 Airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALC</th>\n",
       "      <td>ALCAN, AK [ALC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANC</th>\n",
       "      <td>ANCHORAGE, AK [ANC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAR</th>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK [BAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAC</th>\n",
       "      <td>DALTONS CACHE, AK [DAC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIZ</th>\n",
       "      <td>DEW STATION PT LAY DEW, AK [PIZ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            I94 Airport\n",
       "ALC                     ALCAN, AK [ALC]\n",
       "ANC                 ANCHORAGE, AK [ANC]\n",
       "BAR  BAKER AAF - BAKER ISLAND, AK [BAR]\n",
       "DAC             DALTONS CACHE, AK [DAC]\n",
       "PIZ    DEW STATION PT LAY DEW, AK [PIZ]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class I94Airport:\n",
    "    \"\"\"\n",
    "        I94Airport contains parsed information from `i94port` field\n",
    "        of the `I94 Immigration Data`.\n",
    "        Also it implements static method `sas_description_parse_airport_codes`\n",
    "        which parses SAS descriptions and returns dictionary\n",
    "        with airport codes as keys and I94Aiport objects as values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, airport_code, us_city, us_state):\n",
    "        self._airport_code = airport_code\n",
    "        self._us_city = us_city\n",
    "        self._us_state = us_state\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self._us_city}, {self._us_state} [{self._airport_code}]'\n",
    "\n",
    "    @property\n",
    "    def airport_code(self):\n",
    "        return self._airport_code\n",
    "\n",
    "    @property\n",
    "    def us_city(self):\n",
    "        return self._us_city\n",
    "\n",
    "    @property\n",
    "    def us_state(self):\n",
    "        return self._us_state\n",
    "\n",
    "    @staticmethod\n",
    "    def sas_description_parse_airport_codes(sas_description_filename):\n",
    "        \"\"\"\n",
    "            Load SAS description file, parse `i94port` values,\n",
    "            filter non-U.S., \"Collapsed\" or \"No port\" data\n",
    "            and returns dictionary with parsed results to fast lookups.\n",
    "        \"\"\"\n",
    "\n",
    "        airports = {}\n",
    "        is_airports_section = False\n",
    "\n",
    "        # We will use regex to extract all three parts (code, city and state) from the description file.\n",
    "        # Example: 'LOS' = 'LOS ANGELES, CA       '\n",
    "        # Regex pattern matching: LOS = match[0], LOS ANGELES = match[1], CA = match[2]\n",
    "        # Also current structure of regex eliminates all invalid airport codes.\n",
    "        airport_regex = re.compile(r'\\'([A-Z0-9]{3})\\'.*=.*\\'([A-Z-\\/,\\.\\ ]+),\\ ?([A-Z]{2})\\ *\\'')\n",
    "\n",
    "        with open(sas_description_filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == ';':\n",
    "                    is_airports_section = False\n",
    "                elif line == 'value $i94prtl':\n",
    "                    is_airports_section = True\n",
    "                elif is_airports_section:\n",
    "                    airport_parts = airport_regex.findall(line)\n",
    "                    if airport_parts:\n",
    "                        airport_code, us_city, us_state = airport_parts[0]\n",
    "                        airports[airport_code] = I94Airport(airport_code, us_city, us_state)\n",
    "        return airports\n",
    "\n",
    "\n",
    "# Uncomment following code to test class I94Airport and it's static method sas_description_parse_airport_codes\n",
    "# i94_airports = I94Airport.sas_description_parse_airport_codes('./I94_SAS_Labels_Descriptions.SAS')\n",
    "# i94_airports_df = pd.DataFrame.from_dict(i94_airports, orient='index', columns=['I94 Airport'])\n",
    "# i94_airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+---------+----------+\n",
      "|  cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|cicid_long|i94yr_int|i94mon_int|\n",
      "+-------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+---------+----------+\n",
      "| 2302.0|2016.0|   4.0| 107.0| 107.0|    FMY|20545.0|    1.0|   null|20558.0|  64.0|    1.0|  1.0|20160401|    null| null|      T|      K|   null|      M| 1952.0|09302016|     F|  null|     SY|   6.66572685E8|  514|      B1|      2302|     2016|         4|\n",
      "|12806.0|2016.0|   4.0| 116.0| 116.0|    FMY|20545.0|    1.0|     IL|20549.0|  73.0|    1.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1943.0|06292016|     M|  null|     EI|5.5406005533E10|00123|      WB|     12806|     2016|         4|\n",
      "|13351.0|2016.0|   4.0| 116.0| 116.0|    BGM|20545.0|    1.0|     ME|20547.0|  68.0|    1.0|  1.0|20160401|     DBL| null|      G|      O|   null|      M| 1948.0|09302016|     M|  null|    *GA| 9.250494923E10|MABIS|      B1|     13351|     2016|         4|\n",
      "|26320.0|2016.0|   4.0| 131.0| 131.0|    BGM|20545.0|    1.0|     OH|20552.0|  46.0|    1.0|  1.0|20160401|     BEN| null|      G|      O|   null|      M| 1970.0|09302016|     M|  null|    *GA| 9.244752923E10|VPBMA|      B1|     26320|     2016|         4|\n",
      "|27382.0|2016.0|   4.0| 133.0| 264.0|    FMY|20545.0|    1.0|     IL|20548.0|  42.0|    2.0|  1.0|20160401|     IST| null|      G|      O|   null|      M| 1974.0|09302016|     F|  null|     TK| 9.250723453E10|00005|      B2|     27382|     2016|         4|\n",
      "+-------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_i94_immigration(sas_source_filename, sas_description_filename):\n",
    "    \"\"\"\n",
    "        Loads and cleanup I94 Immigration Data.\n",
    "        Apply cleanup steps:\n",
    "        - i94port should be in the list of valid airports;\n",
    "        - i94mode = 1 (arrived to the U.S. by air);\n",
    "        - gender is not empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load airports from SAS description file\n",
    "    i94_airports = I94Airport.sas_description_parse_airport_codes(sas_description_filename)\n",
    "\n",
    "    # Load raw dataset as SAS using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('com.github.saurfang.sas.spark')\n",
    "        .load(sas_source_filename)\n",
    "    )\n",
    "\n",
    "    # Cleanup data and repartition by `i94port` to speedup future queries\n",
    "    cleaned_df = (\n",
    "        raw_df\n",
    "        .filter(F.col('i94mode') == 1)\n",
    "        .filter(F.col('gender').isNotNull())\n",
    "        .filter(F.col('i94port').isin(list(i94_airports.keys())))\n",
    "        .withColumn('cicid_long', F.col('cicid').cast(LongType()))\n",
    "        .withColumn('i94yr_int', F.col('i94yr').cast(IntegerType()))\n",
    "        .withColumn('i94mon_int', F.col('i94mon').cast(IntegerType()))\n",
    "        .repartition('i94port')\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_i94_immigration function\n",
    "immigration_df = load_and_cleanup_i94_immigration('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', './I94_SAS_Labels_Descriptions.SAS')\n",
    "immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and cleanup Airport Codes\n",
    "\n",
    "There are several steps to cleanup airport codes data:\n",
    "\n",
    "* we are interested only in the U.S. airports, thus we will preserve data by a condition `iso_country` = `US`;\n",
    "* we need only international airports, thus we will filter out all data with empty `iata_code`;\n",
    "* we need only airports belongs to cities (filled `municipality` field);\n",
    "* we need to extract U.S. state code from `iso_region` field, which contains state prefixed by ISO country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|state_code|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|        FL|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|        AK|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|        CO|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|        TX|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|        MA|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_cleanup_airport_codes(source_filename):\n",
    "    \"\"\"\n",
    "        Loads airport codes data from the source file in CSV format.\n",
    "        Apply cleanup steps:\n",
    "        - preserve only U.S. airport codes;\n",
    "        - remove all rows with empty IATA code;\n",
    "        - remove all rows with empty city (municipality);\n",
    "        - add column `state_code` with extracted U.S. state from the `iso_region` field.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load raw dataset as CSV using Spark\n",
    "    raw_df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format('csv')\n",
    "        .option('header', 'true')\n",
    "        .option('delimiter', ',')\n",
    "        .load(source_filename)\n",
    "    )\n",
    "\n",
    "    # Cleanup data\n",
    "    cleaned_df = (\n",
    "        raw_df\n",
    "        .filter(F.col('iso_country') == 'US')\n",
    "        .filter(F.col('iata_code').isNotNull())\n",
    "        .filter(F.col('municipality').isNotNull())\n",
    "        .withColumn('state_code', F.col('iso_region')[-2:2])\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Uncomment following code to test load_and_cleanup_demographic function\n",
    "# airport_codes_df = load_and_cleanup_airport_codes('./airport-codes_csv.csv')\n",
    "# airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Currently analytical area in the Data Lake consists of two dimension tables, one fact table and one aggregated table (materialized view) which could be considered as a part of Data Mart.\n",
    "\n",
    "1. `cities` dimension table contains information about U.S. cities. It is filled from the `U.S. City Demographic Data` data source.\n",
    "  Table structure:\n",
    "  * `city_id` – auto-generated surrogate row identifier;\n",
    "  * `name` – U.S. city name;\n",
    "  * `state_name` – U.S. state name;\n",
    "  * `state_code` – U.S. state code.\n",
    "\n",
    "  `cities` dimension table will be partitioned by `state_code` because it is how it will usually queried.\n",
    "\n",
    "\n",
    "2. `airports` dimension table contains information about U.S. international airports. This table is filled from the `Airport Codes` and actually used to link U.S. cities with the immigration data via the airport code. So here we will use the trick and fill this table from the `Airport Codes` joined with SAS Descripton file for the `I94 Immigration Data` which is much easier then parse whole `I94 Immigration Data` dataset. Also this trick will speedup future steps.\n",
    "  Table structure:\n",
    "  * `airport_id` – auto-generated surrogate row identifier. Actually we could use iata_code as identifier, but we will use numeric IDs for consistency and extensibility, because in future we could add local airports too for some reason;\n",
    "  * `name` – airport name;\n",
    "  * `city` – airport city (could be used to join with `cities`);\n",
    "  * `state_code` – U.S. state code (could be used to join with `cities`);\n",
    "  * `iata_code` – IATA airport code;\n",
    "  * `i94_code` – airport code in I94 data format.\n",
    "\n",
    "  `airports` dimension table will be partitioned by `state_code` for future joins with `cities`.\n",
    "\n",
    "\n",
    "3. `immigration` fact table register immigration events which come from the `I94 Immigration Data` data source. Also it will join information from the `airports` and `cities` to link each immigration event with correct `city_id`.\n",
    "\n",
    "  Table structure:\n",
    "  * `immigration_id` – auto-generated surrogate row identifier;\n",
    "  * `city_id` – reference key to the `cities`.`city_id`;\n",
    "  * `airport_id` – reference key to the `airports`.`airport_id`;\n",
    "  * `year` – 4-digit year;\n",
    "  * `month` – 1-digit month (1 - Jan, 2 - Feb, 3 - Mar, etc.);\n",
    "  * `gender` – 1-character gender (M - Male, F - Female);\n",
    "  * `i94_cicid` – identifier from the `I94 Immigration Data` to allow find and check row in the original data source. Also in could help in the events deduplication procedure;\n",
    "  * `city` – destination U.S. city name (add redundancy to facilitate analytics);\n",
    "  * `state_code` – destination U.S. state code (add redundancy to facilitate analytics);\n",
    "  * `iata_code` – destination IATA airport code (add redundancy to facilitate analytics).\n",
    "\n",
    "  `immigration` fact table will be partitioned by `year` and `city_id` because we will group by fields to fill `demographics` table.\n",
    "\n",
    "\n",
    "4. `demographics` materialized view contains aggregated data from the `immigration` table grouped by year and city. This table filled from the two data sources: `U.S. City Demographic Data` which contains already pre-calculated data for 2015 year and from the `immigration` fact table joined with `cities`.\n",
    "\n",
    "  Table structure:\n",
    "  * `demographics_id` – auto-generated surrogate row identifier;\n",
    "  * `city_id` – reference key to the `cities`.`city_id`;\n",
    "  * `year` – 4-digit year;\n",
    "  * `city` – U.S. city name (add redundancy to facilitate analytics);\n",
    "  * `state_code` – U.S. state code (add redundancy to facilitate analytics);\n",
    "  * `male_population` – male population amount;\n",
    "  * `female_population` – female population amount;\n",
    "  * `total_population` – total population amount.\n",
    "\n",
    "  `demographics` materialized view will be partitioned by `year` and `state_code` because usually to speedup most frequently queries.\n",
    "\n",
    "The overall analytical area database diagram is shown on the following scheme:\n",
    "\n",
    "![capstone_schema](images/capstone_schema.png)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The entire ETL-pipeline contains the following steps:\n",
    "1. Load and cleanup the `U.S. City Demographic Data` (see step 2).\n",
    "2. Load and cleanup the `Airport Codes Data` (see step 2).\n",
    "3. Load and cleanup the `I94 immigration Data` (see step 2).\n",
    "4. Create schema for the `cities` dimension table.\n",
    "5. Fill `cities` dimension table with data and write to the analytical area as a `parquet` file format partitioned by `state_code`.\n",
    "6. Create schema for the `airports` dimension table.\n",
    "7. Fill `airports` dimension table with data and write to the analytical area as a `parquet` file format partitioned by `state_code`.\n",
    "8. Create schema for the `immigration` fact table.\n",
    "9. Fill `immigration` fact table with data and write to the analytical area as a `parquet` file format partitioned by `year` and `city_id`.\n",
    "10. Create schema for the `demographics` materialized view.\n",
    "11. Fill `demographics` materialized view with data and write to the analytical area as a `parquet` file format partitioned by `year` and `state_code`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data\n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ETL-pipeline steps [1-3]: Load and cleanup all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set paths to data sources\n",
    "demographics_filename = './us-cities-demographics.json'\n",
    "airport_codes_filename = './airport-codes_csv.csv'\n",
    "# immigration_sas_filename = '../../data/18-83510-I94-Data-2016/*.sas7bdat'\n",
    "immigration_sas_filename = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_sas_description_filename = './I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "# Load and cleanup data\n",
    "demographic_df = load_and_cleanup_demographic(demographics_filename)\n",
    "airport_codes_df = load_and_cleanup_airport_codes(airport_codes_filename)\n",
    "immigration_df = load_and_cleanup_i94_immigration(immigration_sas_filename, immigration_sas_description_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ETL-pipeline steps [4-5]: Fill `cities` dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create schema for `cities` dimension table to validate data correctness\n",
    "cities_schema = StructType([\n",
    "    StructField('city_id', LongType(), nullable=False),\n",
    "    StructField('name', StringType(), nullable=False),\n",
    "    StructField('state_name', StringType(), nullable=False),\n",
    "    StructField('state_code', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create rdd from the `demographic_df` dataframe selecting required fields\n",
    "cities_rdd = (\n",
    "    demographic_df\n",
    "    .withColumn('city_id', F.monotonically_increasing_id() + 1)\n",
    "    .dropDuplicates(['city', 'state_code'])\n",
    "    .select('city_id', 'city', 'state', 'state_code')\n",
    "    .rdd\n",
    ")\n",
    "\n",
    "# Create `cities` table using prepared data and schema\n",
    "cities_table = spark.createDataFrame(cities_rdd, cities_schema)\n",
    "\n",
    "# Write `cities` dimension table to parquet partitioned by `state_code`\n",
    "cities_table_path = './tables/cities/cities.parquet'\n",
    "(\n",
    "    cities_table\n",
    "    .write\n",
    "    .partitionBy('state_code')\n",
    "    .mode('overwrite')\n",
    "    .parquet(cities_table_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ETL-pipeline steps [6-7]: Fill `airports` dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create schema for `airports` dimension table to validate data correctness\n",
    "airports_schema = StructType([\n",
    "    StructField('airport_id', LongType(), nullable=False),\n",
    "    StructField('name', StringType(), nullable=False),\n",
    "    StructField('city', StringType(), nullable=False),\n",
    "    StructField('state_code', StringType(), nullable=False),\n",
    "    StructField('iata_code', StringType(), nullable=False),\n",
    "    StructField('i94_code', StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Construct dataframe with airport information using SAS description for the `I94 Immigration Data`\n",
    "# using I94Airport class described above (see step 2)\n",
    "i94_airports = I94Airport.sas_description_parse_airport_codes(immigration_sas_description_filename)\n",
    "i94_airports_df = (\n",
    "    spark\n",
    "    .sparkContext\n",
    "    .parallelize(list(i94_airports.keys()))\n",
    "    .map(lambda i94port: Row(**{\n",
    "        'i94_port': i94port.upper(),\n",
    "        'i94_city': i94_airports[i94port].us_city.upper(),\n",
    "        'i94_state_code': i94_airports[i94port].us_state.upper(),\n",
    "    }))\n",
    "    .toDF()\n",
    ")\n",
    "\n",
    "\n",
    "# Create rdd from the `airport_codes_df` dataframe joined with `i94_airports_df` dataframe\n",
    "airports_rdd = (\n",
    "    airport_codes_df\n",
    "    .withColumn('airport_id', F.monotonically_increasing_id() + 1)\n",
    "    .withColumn('city_upper', F.upper(F.col('municipality')))\n",
    "    .join(i94_airports_df,\n",
    "          (F.col('state_code') == i94_airports_df.i94_state_code)\n",
    "          & (F.col('city_upper') == i94_airports_df.i94_city)\n",
    "          , 'left_outer')\n",
    "    .dropDuplicates(['iata_code'])\n",
    "    .select('airport_id', 'name', 'municipality', 'state_code', 'iata_code', 'i94_port')\n",
    "    .rdd\n",
    ")\n",
    "\n",
    "# Create `airports` table using prepared data and schema\n",
    "airports_table = spark.createDataFrame(airports_rdd, airports_schema)\n",
    "\n",
    "# Write `cities` dimension table to parquet partitioned by `state_code`\n",
    "airports_table_path = './tables/airports/airports.parquet'\n",
    "(\n",
    "    airports_table\n",
    "    .write\n",
    "    .partitionBy('state_code')\n",
    "    .mode('overwrite')\n",
    "    .parquet(airports_table_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ETL-pipeline steps [8-9]: Fill `immigration` fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create schema for `immigration` fact table to validate data correctness\n",
    "immigration_schema = StructType([\n",
    "    StructField('immigration_id', LongType(), nullable=False),\n",
    "    StructField('city_id', LongType(), nullable=False),\n",
    "    StructField('airport_id', LongType(), nullable=False),\n",
    "    StructField('year', IntegerType(), nullable=False),\n",
    "    StructField('month', IntegerType(), nullable=False),\n",
    "    StructField('gender', StringType(), nullable=False),\n",
    "    StructField('i94_cicid', LongType(), nullable=False),\n",
    "    StructField('city', StringType(), nullable=False),\n",
    "    StructField('state_code', StringType(), nullable=False),\n",
    "    StructField('iata_code', StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Create rdd from the `immigration_df` dataframe joined with already created airports_table and cities_table dataframes\n",
    "immigration_rdd = (\n",
    "    immigration_df\n",
    "    .join(airports_table,\n",
    "          (immigration_df.i94port == airports_table.i94_code)\n",
    "          , 'inner')\n",
    "    .join(cities_table,\n",
    "          (cities_table.name == airports_table.city)\n",
    "          & (cities_table.state_code == airports_table.state_code)\n",
    "          , 'inner')\n",
    "    .withColumn('immigration_id', F.monotonically_increasing_id() + 1)\n",
    "    .dropDuplicates(['cicid'])\n",
    "    .select(\n",
    "        F.col('immigration_id'),\n",
    "        cities_table.city_id.alias('city_id'),\n",
    "        airports_table.airport_id.alias('airport_id'),\n",
    "        immigration_df.i94yr_int.alias('year'),\n",
    "        immigration_df.i94mon_int.alias('month'),\n",
    "        immigration_df.gender.alias('gender'),\n",
    "        immigration_df.cicid_long.alias('i94_cicid'),\n",
    "        cities_table.name.alias('city'),\n",
    "        cities_table.state_code.alias('state_code'),\n",
    "        airports_table.iata_code.alias('iata_code')\n",
    "    )\n",
    "    .rdd\n",
    ")\n",
    "\n",
    "# Create `immigration` table using prepared data and schema\n",
    "immigration_table = spark.createDataFrame(immigration_rdd, immigration_schema)\n",
    "\n",
    "Write `immigration` fact table to parquet partitioned by `year` and `city_id`\n",
    "immigration_table_path = './tables/immigration/immigration.parquet'\n",
    "(\n",
    "    immigration_table\n",
    "    .write\n",
    "    .partitionBy('year', 'city_id')\n",
    "    # .mode('append') for the sake of the demo capstone project we will use mode=overwrite to save storage space\n",
    "    .mode('overwrite')\n",
    "    .parquet(immigration_table_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### ETL-pipeline steps [10-11]: Fill demographics materialized view¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create schema for `demographics` table to validate data correctness\n",
    "demographics_schema = StructType([\n",
    "    StructField('demographics_id', LongType(), nullable=False),\n",
    "    StructField('city_id', LongType(), nullable=False),\n",
    "    StructField('year', IntegerType(), nullable=False),\n",
    "    StructField('city', StringType(), nullable=False),\n",
    "    StructField('state_code', StringType(), nullable=False),\n",
    "    StructField('male_population', LongType(), nullable=False),\n",
    "    StructField('female_population', LongType(), nullable=False),\n",
    "    StructField('total_population', LongType(), nullable=False)\n",
    "])\n",
    "\n",
    "# 1. We need to load data for 2015 year from the U.S. City Demographic Data\n",
    "# as a basis for the future years cumulative data.\n",
    "# For this task we have to join `demographic_df` with `cities_table`\n",
    "# and group by data by `state_code` and `city`.\n",
    "demographics_2015_df = (\n",
    "    demographic_df\n",
    "        .join(cities_table,\n",
    "          (cities_table.name == demographic_df.city)\n",
    "          & (cities_table.state_code == demographic_df.state_code)\n",
    "          , 'inner')\n",
    "    .groupBy([demographic_df.state_code, demographic_df.city, cities_table.city_id])\n",
    "    .agg({'male_population': 'sum', 'female_population': 'sum', 'total_population': 'sum'})\n",
    "    .withColumn('year', F.lit(2015))\n",
    "    .select(\n",
    "        cities_table.city_id.alias('city_id'),\n",
    "        F.col('year'),\n",
    "        demographic_df.city.alias('city'),\n",
    "        demographic_df.state_code.alias('state_code'),\n",
    "        F.col('sum(male_population)').alias('male_population'),\n",
    "        F.col('sum(female_population)').alias('female_population'),\n",
    "        F.col('sum(total_population)').alias('total_population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Also we need to load data from the `immigration_table` grouped by `year`, `state_code` and `city`.\n",
    "demographics_next_df = (\n",
    "    immigration_table\n",
    "    .withColumn('male_population', F.when(F.col('gender') == 'M', 1).otherwise(0))\n",
    "    .withColumn('female_population', F.when(F.col('gender') == 'F', 1).otherwise(0))\n",
    "    .withColumn('total_population', F.lit(1))\n",
    "    .groupBy('year', 'state_code', 'city', 'city_id')\n",
    "    .agg({'male_population': 'sum', 'female_population': 'sum', 'total_population': 'sum'})\n",
    "    .select(\n",
    "        'city_id',\n",
    "        'year',\n",
    "        'city',\n",
    "        'state_code',\n",
    "        F.col('sum(male_population)').alias('male_population'),\n",
    "        F.col('sum(female_population)').alias('female_population'),\n",
    "        F.col('sum(total_population)').alias('total_population')\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Finally we should combine two previous results to one dataset.\n",
    "# The tricky part here is to add results of the previous year to the next year,\n",
    "# in other words we want to get cumulative results each next year depending on previous year value.\n",
    "# We use Window functions to achive this goal.\n",
    "demographics_window = (\n",
    "    Window\n",
    "    .partitionBy('city_id')\n",
    "    .orderBy(F.asc('year'))\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "demographics_rdd = (\n",
    "    demographics_2015_df\n",
    "    .unionAll(demographics_next_df)\n",
    "    .withColumn('cumulative_male_population', F.sum('male_population').over(demographics_window))\n",
    "    .withColumn('cumulative_female_population', F.sum('female_population').over(demographics_window))\n",
    "    .withColumn('cumulative_total_population', F.sum('total_population').over(demographics_window))\n",
    "    .withColumn('demographics_id', F.monotonically_increasing_id() + 1)\n",
    "    .select(\n",
    "        'demographics_id',\n",
    "        'city_id',\n",
    "        'year',\n",
    "        'city',\n",
    "        'state_code',\n",
    "        'cumulative_male_population',\n",
    "        'cumulative_female_population',\n",
    "        'cumulative_total_population'\n",
    "    )\n",
    "    .rdd\n",
    ")\n",
    "\n",
    "# Create `demographics` table using prepared data and schema\n",
    "demographics_table = spark.createDataFrame(demographics_rdd, demographics_schema)\n",
    "\n",
    "# Write `demographics` table to parquet partitioned by `year` and `state_code`\n",
    "demographics_table_path = './tables/demographics/demographics.parquet'\n",
    "(\n",
    "    demographics_table\n",
    "    .write\n",
    "    .partitionBy('year', 'state_code')\n",
    "    .mode('overwrite')\n",
    "    .parquet(demographics_table_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "There are two types of data quality checks:\n",
    "\n",
    "1. Each table should not be empty.\n",
    "2. We will test `demographics` table for a few major U.S. city for 2015 and 2016 years. We expect that population in 2016 should be higher than in 2015. It is because that our project counts only immigrations to the U.S. and does not take into account any other population changes, thus population can only growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Perform data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performs data quality check. Tests table \"cities\" to be not empty...\n",
      "Data quality check passed. Table \"cities\" has 596 rows.\n",
      "Performs data quality check. Tests table \"airports\" to be not empty...\n",
      "Data quality check passed. Table \"airports\" has 2011 rows.\n",
      "Performs data quality check. Tests table \"immigration\" to be not empty...\n",
      "Data quality check passed. Table \"immigration\" has 2053055 rows.\n",
      "Performs data quality check. Tests table \"demographics\" to be not empty...\n",
      "Data quality check passed. Table \"demographics\" has 661 rows.\n",
      "Performs data quality check. Tests demographics table \"demographics\" that population should grows...\n",
      "Data quality check passed. Table \"demographics\" has positive population changes between 2015 and 2016 years.\n"
     ]
    }
   ],
   "source": [
    "def table_is_not_empty_check(table_df, table_name):\n",
    "    \"\"\"Data quality check. It tests that table is not empty.\"\"\"\n",
    "\n",
    "    print(f'Performs data quality check. Tests table \"{table_name}\" to be not empty...')\n",
    "\n",
    "    row_count = table_df.count()\n",
    "    assert row_count > 0, f'Data quality check failed. Table \"{table_name}\" is empty.'\n",
    "\n",
    "    print(f'Data quality check passed. Table \"{table_name}\" has {row_count} rows.')\n",
    "\n",
    "\n",
    "def population_changes_check(demographics_table_df, demographics_table_name):\n",
    "    \"\"\"\n",
    "        Data quality check.\n",
    "        It takes some major U.S. cities and compare population in 2015 and 2016 years.\n",
    "        Population in 2016 should be greater for each city, because the only population change\n",
    "        that we take into account is immigration to the U.S.\n",
    "        Also we expect that in major cities at least few peoples will come each year.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Performs data quality check. Tests demographics table \"{demographics_table_name}\" that population should grows...')\n",
    "\n",
    "    cities = ['Los Angeles, CA', 'New York, NY', 'San Francisco, CA', 'Washington, DC']\n",
    "\n",
    "    demographics_pd_df = (\n",
    "        demographics_table_df\n",
    "        .filter(F.col('year').isin([2015, 2016]))\n",
    "        .withColumn('city_with_state', F.concat_ws(', ', F.col('city'), F.col('state_code')))\n",
    "        .filter(F.col('city_with_state').isin(cities))\n",
    "        .select('year', 'city_with_state', 'total_population')\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    demographics_2015_pd_df = demographics_pd_df[demographics_pd_df['year'] == 2015].copy()\n",
    "    demographics_2016_pd_df = demographics_pd_df[demographics_pd_df['year'] == 2016].copy()\n",
    "    demographics_2016_pd_df['diff'] = demographics_2016_pd_df['total_population'] - demographics_2015_pd_df['total_population'].values\n",
    "\n",
    "    for _, row in demographics_2016_pd_df.iterrows():\n",
    "        city = row['city_with_state']\n",
    "        diff = row['diff']\n",
    "        assert diff > 0, \\\n",
    "            f'Data quality check failed. Table \"{demographics_table_name}\" has negative population changes between 2015 and 2016 for \"{city}\".'\n",
    "\n",
    "    print(f'Data quality check passed. Table \"{demographics_table_name}\" has positive population changes between 2015 and 2016 years.')\n",
    "\n",
    "\n",
    "# Perform data quality checks\n",
    "table_is_not_empty_check(cities_table, 'cities')\n",
    "table_is_not_empty_check(airports_table, 'airports')\n",
    "table_is_not_empty_check(immigration_table, 'immigration')\n",
    "table_is_not_empty_check(demographics_table, 'demographics')\n",
    "population_changes_check(demographics_table, 'demographics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Data model consists of four tables:\n",
    "\n",
    "* `cities` – dimension table which contains information about U.S. cities;\n",
    "* `airports` – dimension table which contains information about U.S. airports;\n",
    "* `immigration` – fact table which register facts about immigration to U.S.;\n",
    "* `demographics` – materialized view which describe population changes in the U.S. cities.\n",
    "\n",
    "Actually data dictionary was already partially described in the step `3.1. Conceptual Data Model` (see step 3.1). Thus here we will provide summary on each table and each field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data dictionary for `cities`\n",
    "\n",
    "`cities` dimension table contains information about U.S. cities. It is filled from the `U.S. City Demographic Data` data source.\n",
    "\n",
    "Table structure:\n",
    "  * `city_id` – auto-generated surrogate row identifier. It is easier for practice usage to reference by numeric ID;\n",
    "  * `name` – U.S. city name;\n",
    "  * `state_name` – U.S. state name;\n",
    "  * `state_code` – U.S. state code.\n",
    "\n",
    "The main table purpose is to keep up to date dictionary of the U.S. cities for which we want analyze demogrphics situation.\n",
    "\n",
    "Print a few rows from `cities` dimension table to show the data in the resulting database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "|airport_id|                name|      city|state_code|iata_code|i94_code|\n",
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "|       331|Greater Binghamto...|Binghamton|        NY|      BGM|    null|\n",
      "|        20|      Eagle Air Park|  Brazoria|        TX|      BZT|    null|\n",
      "|       488|Chanute Martin Jo...|   Chanute|        KS|      CNU|    null|\n",
      "|       510|C David Campbell ...| Corsicana|        TX|      CRS|    null|\n",
      "|       715|          Page Field|Fort Myers|        FL|      FMY|     FMY|\n",
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `citites` data example\n",
    "airports_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data dictionary for `airports`\n",
    "\n",
    "`airports` dimension table contains information about U.S. international airports. This table is filled from the `Airport Codes` and actually used to link U.S. cities with the immigration data via the airport code.\n",
    "\n",
    "Table structure:\n",
    "* `airport_id` – auto-generated surrogate row identifier. Actually we could use iata_code as identifier, but we will use numeric IDs for consistency and extensibility, because in future we could add local airports too for some reason;\n",
    "* `name` – airport name;\n",
    "* `city` – airport city;\n",
    "* `state_code` – U.S. state code;\n",
    "* `iata_code` – IATA airport code;\n",
    "* `i94_code` – airport code in I94 data format.\n",
    "\n",
    "The main table purpose is to keep up to date dictionary of the U.S. international airports. It will be used to link immigration data with destination cities.\n",
    "\n",
    "Print a few rows from `airports` dimension table to show the data in the resulting database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "|airport_id|                name|      city|state_code|iata_code|i94_code|\n",
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "|       331|Greater Binghamto...|Binghamton|        NY|      BGM|    null|\n",
      "|        20|      Eagle Air Park|  Brazoria|        TX|      BZT|    null|\n",
      "|       488|Chanute Martin Jo...|   Chanute|        KS|      CNU|    null|\n",
      "|       510|C David Campbell ...| Corsicana|        TX|      CRS|    null|\n",
      "|       715|          Page Field|Fort Myers|        FL|      FMY|     FMY|\n",
      "+----------+--------------------+----------+----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `airports` data example\n",
    "airports_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data dictionary for `immigration`\n",
    "\n",
    "`immigration` fact table register immigration events which come from the `I94 Immigration Data` data source. Also it will join information from the `airports` and `cities` to link each immigration event with correct city.\n",
    "\n",
    "Table structure:\n",
    "* `immigration_id` – auto-generated surrogate row identifier;\n",
    "* `city_id` – reference key to the `cities`.`city_id`;\n",
    "* `airport_id` – reference key to the `airports`.`airport_id`;\n",
    "* `year` – 4-digit year;\n",
    "* `month` – 1-digit month (1 - Jan, 2 - Feb, 3 - Mar, etc.);\n",
    "* `gender` – 1-character gender (M - Male, F - Female);\n",
    "* `i94_cicid` – identifier from the `I94 Immigration Data` to allow find and check row in the original data source. Also in could help in the events deduplication procedure;\n",
    "* `city` – destination U.S. city name;\n",
    "* `state_code` – destination U.S. state code;\n",
    "* `iata_code` – destination IATA airport code.\n",
    "\n",
    "The core table in our database. Actual immigration events are registered in this table.\n",
    "\n",
    "Print a few rows from `immigration` fact table to show the data in the resulting database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------+----+-----+------+---------+-------------+----------+---------+\n",
      "|immigration_id|city_id|airport_id|year|month|gender|i94_cicid|         city|state_code|iata_code|\n",
      "+--------------+-------+----------+----+-----+------+---------+-------------+----------+---------+\n",
      "|  274877906948|    472|      1484|2016|    4|     M|      558|San Francisco|        CA|      SFO|\n",
      "|  558345748553|    119|      1987|2016|    4|     M|     2862|       Boston|        MA|      BNH|\n",
      "| 1357209665577|    273|       477|2016|    4|     F|     3901|    Charlotte|        NC|      CLT|\n",
      "| 1022202221979|    701|       144|2016|    4|     F|     4066|     New York|        NY|      JRB|\n",
      "| 1022202222917|    701|       144|2016|    4|     F|     5858|     New York|        NY|      JRB|\n",
      "+--------------+-------+----------+----+-----+------+---------+-------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `immigration` data example\n",
    "immigration_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data dictionary for `demographics`\n",
    "\n",
    "`demographics` materialized view contains aggregated data from the `immigration` table grouped by year and city. This table filled from the two data sources: `U.S. City Demographic Data` which contains already pre-calculated data for 2015 year and from the `immigration` fact table joined with `cities`.\n",
    "\n",
    "Table structure:\n",
    "* `demographics_id` – auto-generated surrogate row identifier;\n",
    "* `city_id` – reference key to the `cities`.`city_id`;\n",
    "* `year` – 4-digit year;\n",
    "* `city` – U.S. city name (add redundancy to facilitate analytics);\n",
    "* `state_code` – U.S. state code (add redundancy to facilitate analytics);\n",
    "* `male_population` – male population amount;\n",
    "* `female_population` – female population amount;\n",
    "* `total_population` – total population amount.\n",
    "\n",
    "Aggreagate materialized view depends on immigration data. It is used for analytics and reporting.\n",
    "\n",
    "Print a few rows from `demographics` table to show the data in the resulting database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----+----------+----------+---------------+-----------------+----------------+\n",
      "|demographics_id|city_id|year|      city|state_code|male_population|female_population|total_population|\n",
      "+---------------+-------+----+----------+----------+---------------+-----------------+----------------+\n",
      "|              1|     26|2015|Cedar Park|        TX|         156240|           167945|          324185|\n",
      "|              2|     29|2015|   Fremont|        CA|         571915|           589040|         1160955|\n",
      "|     8589934593|     65|2015|Greensboro|        NC|         661255|           765465|         1426720|\n",
      "|     8589934594|    191|2015|     Tampa|        FL|         877585|           967555|         1845140|\n",
      "|     8589934595|    191|2016|     Tampa|        FL|         886986|           977231|         1864221|\n",
      "+---------------+-------+----+----------+----------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `demographics` data example\n",
    "demographics_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Q: **Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "A: The core descision is to build Data Lake architecure and to store resulting database in the Data Lake as a parquet files. It allows us to keep raw and prepared for analytics data in one place, also it allows to our analytics team to search insights in the raw data and get benfits from both raw and pre-aggregated data too. As a tool to process raw data and build resulting database was choosen [Apache Spark](https://spark.apache.org/). It is powerful tool that allows us to read with different data formats, loads, transform and aggregate data. The very important that Apache Spark uses lazy evaluation which allows us express complicated transformations in multiple steps and also get benefits from Catalyst optimizer and execute it in the entire data collection operation.\n",
    "\n",
    "Q: **Propose how often the data should be updated and why.**\n",
    "A: It depends of our goals. If we mostly want to work with `demographics` table which contains data partitioned by year we could update it yearly.\n",
    "But here we can have one problem: if raw immigration data comes very frequently then yearly pipeline means processing very large amount of data which can take ages. Maybe we should consider to use stream or micro-batch processing to fill `immigration` table and then update `demographics` using batches at discrete points in time more frequently.\n",
    "\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios\n",
    "\n",
    "Q: **The data was increased by 100x.**\n",
    "A: In this case we possibly should replace batch processing to stream processing using Spark Streaming or other tools. Thus we can update our `immigration` fact table nearly real-time. Also we should update `demographics` data more frequently. For example we can update it monthly. It means that we will read `immigration` data only for the last month which reduce amount of processed data in one batch. Also we wil update `demographics` by adding month by month data.\n",
    "\n",
    "Q: **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "A: Actually if we want to populate a dashboard and we want to see changes each day, then we need some kind of orchestration tool to run our ETL-pipeline by schedule. For example [Apache Airflow](https://airflow.apache.org/) is a great tool for this purpose which allows us to run our pipeline daily.\n",
    "\n",
    "Q: **The database needed to be accessed by 100+ people.**\n",
    "A: First what we can do in this situation is to store our parquet files in the HDFS and give read access to our users. Also, if we have a lot of analytics who want to work only with aggragated data than we can create Data Mart for them, maybe to take some relational database (for example MS SQL Server) and load our `demographics` table into it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}